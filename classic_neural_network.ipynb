{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic Neural Network for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing sentences...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "# Load data\n",
    "train_transcriptions_df = pd.read_csv('train_transcriptions_df.csv')\n",
    "train_labels_df = pd.read_csv('training_labels_df.csv')\n",
    "test_transcriptions_df = pd.read_csv('test_transcriptions_df.csv')\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocess_sentence(text):\n",
    "    text = re.sub(r'<[^>]+>', ' ', text) # Remove content within <>\n",
    "    text = re.sub(r'\\s+', ' ', text)     # Replace multiple spaces with a single space\n",
    "    text = text.strip()                  # Remove leading and trailing spaces\n",
    "    test = text.lower()                  # Convert to lowercase to maintain consistency\n",
    "    return text\n",
    "\n",
    "print(\"Preprocessing sentences...\")\n",
    "\n",
    "train_transcriptions_df['clean_text'] = train_transcriptions_df['text'].apply(preprocess_sentence)\n",
    "test_transcriptions_df['clean_text'] = test_transcriptions_df['text'].apply(preprocess_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeeding of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "bert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "sentences_embeeded = bert.encode(train_transcriptions_df['clean_text'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of test and train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sentences_embeeded, train_labels_df['label'].to_numpy(), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 100, Loss: 0.4692472831904888\n",
      "Epoch: 1, Batch: 200, Loss: 0.3612713104486465\n",
      "Epoch: 1, Batch: 300, Loss: 0.35524294659495353\n",
      "Epoch: 1, Batch: 400, Loss: 0.3487639807164669\n",
      "Epoch: 1, Batch: 500, Loss: 0.3465275876224041\n",
      "Epoch: 1, Batch: 600, Loss: 0.33797567665576933\n",
      "Epoch: 1, Batch: 700, Loss: 0.3400791721045971\n",
      "Epoch: 1, Batch: 800, Loss: 0.3481330679357052\n",
      "Epoch: 1, Batch: 900, Loss: 0.3158370053768158\n",
      "Epoch: 1, Batch: 1000, Loss: 0.31555160999298093\n",
      "Epoch: 1, Batch: 1100, Loss: 0.3148860946297646\n",
      "Epoch: 1, Batch: 1200, Loss: 0.3270181677490473\n",
      "Epoch: 1, Batch: 1300, Loss: 0.32705342963337897\n",
      "Epoch: 1, Batch: 1400, Loss: 0.33480315923690795\n",
      "Epoch: 1, Batch: 1500, Loss: 0.3198452538251877\n",
      "Epoch: 1, Batch: 1600, Loss: 0.31667259708046913\n",
      "Epoch: 1, Batch: 1700, Loss: 0.326815539598465\n",
      "Epoch: 1, Batch: 1800, Loss: 0.33223134562373163\n",
      "Epoch: 2, Batch: 100, Loss: 0.3100523456931114\n",
      "Epoch: 2, Batch: 200, Loss: 0.3091162486374378\n",
      "Epoch: 2, Batch: 300, Loss: 0.3336464919149876\n",
      "Epoch: 2, Batch: 400, Loss: 0.3279773287475109\n",
      "Epoch: 2, Batch: 500, Loss: 0.32932645112276077\n",
      "Epoch: 2, Batch: 600, Loss: 0.3219557186961174\n",
      "Epoch: 2, Batch: 700, Loss: 0.3258216381072998\n",
      "Epoch: 2, Batch: 800, Loss: 0.334831600189209\n",
      "Epoch: 2, Batch: 900, Loss: 0.3037549330294132\n",
      "Epoch: 2, Batch: 1000, Loss: 0.3054113402962685\n",
      "Epoch: 2, Batch: 1100, Loss: 0.3053476332128048\n",
      "Epoch: 2, Batch: 1200, Loss: 0.3174566512554884\n",
      "Epoch: 2, Batch: 1300, Loss: 0.3202785749733448\n",
      "Epoch: 2, Batch: 1400, Loss: 0.3220180432498455\n",
      "Epoch: 2, Batch: 1500, Loss: 0.31065090104937554\n",
      "Epoch: 2, Batch: 1600, Loss: 0.30689510390162467\n",
      "Epoch: 2, Batch: 1700, Loss: 0.3181024873256683\n",
      "Epoch: 2, Batch: 1800, Loss: 0.32234629705548284\n",
      "Epoch: 3, Batch: 100, Loss: 0.3073697876930237\n",
      "Epoch: 3, Batch: 200, Loss: 0.3088884973526001\n",
      "Epoch: 3, Batch: 300, Loss: 0.33225101307034494\n",
      "Epoch: 3, Batch: 400, Loss: 0.3270945143699646\n",
      "Epoch: 3, Batch: 500, Loss: 0.3286059983074665\n",
      "Epoch: 3, Batch: 600, Loss: 0.3194474928081036\n",
      "Epoch: 3, Batch: 700, Loss: 0.3229836343228817\n",
      "Epoch: 3, Batch: 800, Loss: 0.3350234754383564\n",
      "Epoch: 3, Batch: 900, Loss: 0.30291495501995086\n",
      "Epoch: 3, Batch: 1000, Loss: 0.3044285053014755\n",
      "Epoch: 3, Batch: 1100, Loss: 0.3044684615731239\n",
      "Epoch: 3, Batch: 1200, Loss: 0.3154518948495388\n",
      "Epoch: 3, Batch: 1300, Loss: 0.31850977525115015\n",
      "Epoch: 3, Batch: 1400, Loss: 0.3206291833519936\n",
      "Epoch: 3, Batch: 1500, Loss: 0.3085313390940428\n",
      "Epoch: 3, Batch: 1600, Loss: 0.3048748356103897\n",
      "Epoch: 3, Batch: 1700, Loss: 0.3174197643250227\n",
      "Epoch: 3, Batch: 1800, Loss: 0.3198952913284302\n"
     ]
    }
   ],
   "source": [
    "## neural network\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # Input to first hidden layer\n",
    "        self.layer1 = nn.Linear(input_size, 1000) \n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        # Second hidden layer\n",
    "        self.layer2 = nn.Linear(1000, 1000)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(1000, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# Create the model\n",
    "\n",
    "model = NeuralNetwork(X_train.shape[1])\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "# Convert the data into PyTorch tensors\n",
    "\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float().unsqueeze(1)\n",
    "\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "y_test_tensor = torch.from_numpy(y_test).float().unsqueeze(1)\n",
    "\n",
    "# Create the dataloaders\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # Get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print(f'Epoch: {epoch + 1}, Batch: {i + 1}, Loss: {running_loss / 100}')\n",
    "            running_loss = 0.0\n",
    "    scheduler.step()\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        y_pred.extend(outputs.squeeze(1).tolist())\n",
    "        y_true.extend(labels.squeeze(1).tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.82      0.87     11887\n",
      "         1.0       0.47      0.75      0.58      2638\n",
      "\n",
      "    accuracy                           0.80     14525\n",
      "   macro avg       0.70      0.78      0.73     14525\n",
      "weighted avg       0.85      0.80      0.82     14525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_nn = np.array(y_pred) >= 0.29\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "print(classification_report(y_true, y_pred_nn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
