{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Term Memory Neural Network for binary classification of messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have selectied LSTM Neural network as our best model for our message classification model because it is well suited for memorising previous messages and their links between them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing sentences...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "def preprocess_sentence(text):\n",
    "    text = re.sub(r'<[^>]+>', ' ', text) # Remove content within <>\n",
    "    text = re.sub(r'\\s+', ' ', text)     # Replace multiple spaces with a single space\n",
    "    text = text.strip()                  # Remove leading and trailing spaces\n",
    "    text = text.lower()                  # Convert to lowercase to maintain consistency\n",
    "    return text\n",
    "\n",
    "\n",
    "# Load data\n",
    "train_transcriptions_df = pd.read_csv('train_transcriptions_df.csv')\n",
    "train_labels_df = pd.read_csv('training_labels_df.csv')\n",
    "test_transcriptions_df = pd.read_csv('test_transcriptions_df.csv')\n",
    "train_correspondances_df = pd.read_csv('train_correspondances_df.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Preprocessing sentences...\")\n",
    "\n",
    "train_transcriptions_df['clean_text'] = train_transcriptions_df['text'].apply(preprocess_sentence)\n",
    "test_transcriptions_df['clean_text'] = test_transcriptions_df['text'].apply(preprocess_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeedings using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "bert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "sentences_embeeded = bert.encode(train_transcriptions_df['clean_text'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add previous links using indicative vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = train_correspondances_df['type'].unique()\n",
    "\n",
    "vector_dict = {}\n",
    "base_vector = np.zeros(len(links))\n",
    "for i,link in enumerate(links):\n",
    "    link_vector = base_vector.copy()\n",
    "    link_vector[i] = 1\n",
    "    vector_dict[link] = link_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_links_vectors = []\n",
    "# iter over all rows of train_transcriptions_df\n",
    "from tqdm import tqdm\n",
    "for index, row in train_transcriptions_df.iterrows():\n",
    "    correspondances = train_correspondances_df[(train_correspondances_df['transcription_id'] == row['transcription_id'])&(train_correspondances_df['2'] == row['index'])]\n",
    "    link_vector = np.zeros(len(links))\n",
    "    for type_of_link in correspondances['type']:\n",
    "        link_vector += vector_dict[type_of_link]\n",
    "    lst_links_vectors.append(link_vector)\n",
    "lst_links_vectors = np.array(lst_links_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/mathiasgrau/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.2263},\n",
       "       {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       "       {'neg': 0.0, 'neu': 0.826, 'pos': 0.174, 'compound': 0.2732}, ...,\n",
       "       {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4019},\n",
       "       {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n",
       "       {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.296}],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "# Initialize the sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to calculate sentiment scores for each text\n",
    "def get_sentiment_scores(text):\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "    return sentiment_scores\n",
    "\n",
    "# Get the sentiment scores for each text\n",
    "sentiment_scores = train_transcriptions_df['clean_text'].apply(get_sentiment_scores)\n",
    "# transform to array\n",
    "sentiment_scores.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_vectors = []\n",
    "for sentiment_score in sentiment_scores:\n",
    "    sentiment_vector = np.array([sentiment_score['neg'], sentiment_score['neu'], sentiment_score['pos'], sentiment_score['compound']])\n",
    "    sentiment_vectors.append(sentiment_vector)\n",
    "sentiment_vectors = np.array(sentiment_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenation of all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72623, 404)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_features = [np.concatenate([embedding, sentiment, link])\n",
    "                     for embedding, sentiment, link in zip(sentences_embeeded, sentiment_vectors, lst_links_vectors)]\n",
    "\n",
    "combined_features = np.array(combined_features)\n",
    "combined_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72617, 7, 404)\n",
      "(72617,)\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(4, len(combined_features) - 2):\n",
    "    X_train.append(combined_features[i-4:i+3])\n",
    "    y_train.append(train_labels_df['label'].to_numpy()[i])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float().unsqueeze(1)\n",
    "\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "y_test_tensor = torch.from_numpy(y_test).float().unsqueeze(1)\n",
    "\n",
    "# Create the dataloaders\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short Term Memory neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        # Apply sigmoid activation\n",
    "        return torch.sigmoid(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.5845970859351769 for epoch:  1 with threshold:  0.2799999999999999\n",
      "f1 score:  0.5922509225092252 for epoch:  2 with threshold:  0.2799999999999999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def get_best_threshold(y_1,y_2):\n",
    "    best_score = 0.\n",
    "    best_thresold = 0.\n",
    "    for threshold in np.arange(0.1, 0.5, 0.01):\n",
    "        score = f1_score(y_1, y_2 >= threshold)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_thresold = threshold\n",
    "    return best_thresold, best_score\n",
    "        \n",
    "from sklearn.metrics import f1_score\n",
    "# Create the model instance\n",
    "model = LSTMClassifier(input_size=404, hidden_size=1000, num_layers=1, output_size=1)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "\n",
    "# Train the model\n",
    "epochs = 2\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            running_loss = 0.0\n",
    "    scheduler.step()\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            y_pred.extend(outputs.squeeze(1).tolist())\n",
    "            y_true.extend(labels.squeeze(1).tolist())\n",
    "    \n",
    "    threshold, score = get_best_threshold(y_true, y_pred)\n",
    "    print(\"f1 score: \", score, \"for epoch: \", epoch + 1, \"with threshold: \", threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        y_pred.extend(outputs.squeeze(1).tolist())\n",
    "        y_true.extend(labels.squeeze(1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.84      0.88     11835\n",
      "         1.0       0.50      0.72      0.59      2689\n",
      "\n",
      "    accuracy                           0.82     14524\n",
      "   macro avg       0.72      0.78      0.74     14524\n",
      "weighted avg       0.85      0.82      0.83     14524\n",
      "\n"
     ]
    }
   ],
   "source": [
    "threshold, score = get_best_threshold(y_true, y_pred)\n",
    "y_pred_nn = np.array(y_pred) >= threshold\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "print(classification_report(y_true, y_pred_nn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
